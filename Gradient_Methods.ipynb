{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Methods\n",
    "\n",
    "### Method of Steepest Descent\n",
    "\n",
    "This method is fairly simple. We simply iterate in the opposite direction of the gradient according to some parameter $\\lambda_k$. The method is described here:\n",
    "$$x_{n+1} = x_{n} - \\lambda_k \\times \\nabla f(\\bold{x})$$\n",
    "The question is, how do we define $\\lambda_{k}$? One possible way is the Armijio rule. This rule sets $\\lambda_k$ equal to some constant $\\alpha^n$ such that $|f(x_{n+1} - f(x_{n})| \\leq threshold$. The code below implements the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ex (generic function with 1 method)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using Random\n",
    "\n",
    "function armijo(f::Function, guess::Vector{Float64}; beta::Float64 = 0.9, \n",
    "    threshold::Float64 = 1e-8, tol::Float64 = 0.001, max_iter::Int64 = 50)\n",
    "\n",
    "    gradient = x -> ForwardDiff.gradient(f, x)\n",
    "    \n",
    "    #Intialize Guess_Array\n",
    "    Guess_Array = Array{Any}(undef, max_iter+1)\n",
    "    Guess_Array[1,1] = guess\n",
    "\n",
    "    for i in 2:max_iter+1\n",
    "        j::Int64 = 0\n",
    "        val = f(Guess_Array[i-1])\n",
    "        grad = gradient(Guess_Array[i-1])\n",
    "        beta_n::Float64 = 1.0\n",
    "        guess = Guess_Array[i-1] - grad\n",
    "        while true\n",
    "            if f(Guess_Array[i-1])-f(guess) >= threshold\n",
    "                break\n",
    "            end\n",
    "            j += 1\n",
    "            guess = Guess_Array[i-1] - beta ^ j .* grad\n",
    "            if j >= 50\n",
    "                println(\"\"\"Iteration stalled \\n\n",
    "                           Current Derivative Error: $(norm(gradient(guess),2))\\n\n",
    "                           Current Guess: $(guess)\\n\n",
    "                        \"\"\")\n",
    "                return Guess_Array[1:i-1]\n",
    "            end\n",
    "        end\n",
    "        Guess_Array[i] = guess\n",
    "        if norm(gradient(guess),2) <= tol\n",
    "            return Guess_Array[1:i]\n",
    "        end\n",
    "    end\n",
    "    println(\"\"\"Tolerance not achieved\\n \n",
    "    Current Derivative Error: $(norm(Guess_Array[end],2))\\n\n",
    "    Current Guess: $(Guess_Array[end])\\n\n",
    "    \"\"\")\n",
    "    return Guess_Array\n",
    "end\n",
    "\n",
    "function ex(x::Vector)\n",
    "    k = x'*x\n",
    "    return exp(k)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8108464223615186, 1.4912905211036418, 1.2237452293613427, 1.029131280571205, 0.8335327368108912, 0.7637726768186607, 0.6909971754049301, 0.6244835319759886, 0.5857997867881243, 0.49758402227999116, 0.4317051138901623, 0.4109335231417902, 0.37724448808648803, 0.3273574664051822, 0.26294898033546915, 0.24424262571485447, 0.22241843963393362, 0.19823827574230615, 0.17289261249984375, 0.147757066699596, 0.12407603997195384, 0.10272567463866702, 0.08414210598869953, 0.0683897805624674, 0.055288939187099206, 0.044535836463664435, 0.035787828505844346, 0.02871282033501706, 0.023012882711552475, 0.01843224940058789, 0.014757073605259999, 0.011811444112811003, 0.009452121567887606, 0.007563217381060926, 0.006051352666711962, 0.0048414810093168085, 0.0038733890810771217, 0.003098815869064706, 0.002479106257883114, 0.0019833124321039603, 0.0015866639882584847, 0.0012693383805911598, 0.001015474385805815, 0.0008123813935070427, 0.0006499060798617164, 0.0005199253580002299, 0.00041594053938564455]\n"
     ]
    }
   ],
   "source": [
    "nums = 2 .* rand(Float64, 10) .- 1\n",
    "nums = nums/norm(nums,10)\n",
    "println([norm(x,2) for x in armijo(ex,nums; max_iter = 1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code above shows a steady decrease in the error as expected. \n",
    "\n",
    "### Line Search Methods And Descent Direction\n",
    "\n",
    "A vector $d$ is said to be in the descent direction if $\\nabla f(x)^T d < 0$. For the steepest descent method, we have that $d = [-1, -1, -1, \\cdots -1]^T$. For newtons method, we have $d = -1 \\times \\nabla^2f(x_{n})^{-1} \\nabla f(x)$. Howerver, if the hessian is not semipositive definite, the newton step may fail to be a descent direction. In general, a method that satisfies the armijo rule with the descent direction satisfying $d = -H^{-1}_c \\nabla f(x)$ where $H_{c}$ is a model Hessian that is semi-positive definite will converge. The next algorithm will use the hessian if it is positive definite and the identity otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo_alternating (generic function with 1 method)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function armijo_alternating(f::Function, guess::Vector{Float64}; beta::Float64 = 0.9, \n",
    "    threshold::Float64 = 1e-8, tol::Float64 = 0.001, max_iter::Int64 = 50)\n",
    "\n",
    "    gradient = x -> ForwardDiff.gradient(f, x)\n",
    "    hessian = x -> ForwardDiff.hessian(f, x)\n",
    "    \n",
    "    #Intialize Guess_Array\n",
    "    Guess_Array = Array{Any}(undef, max_iter+1)\n",
    "    Guess_Array[1,1] = guess\n",
    "    Newton_Steps::Int64 = 0\n",
    "    Gradient_Steps::Int64 = 0\n",
    "\n",
    "    for i in 2:max_iter+1\n",
    "        H = hessian(guess)\n",
    "        spd_bool = true\n",
    "        eig_vals = eigvals(H)\n",
    "        if minimum(eig_vals) <= 0.0\n",
    "            spd_bool = false \n",
    "        end\n",
    "        #Newton Step\n",
    "        if spd_bool == true\n",
    "            Newton_Steps += 1\n",
    "            n::Int64 = 0\n",
    "            val = f(Guess_Array[i-1])\n",
    "            grad = gradient(Guess_Array[i-1])\n",
    "            beta_n::Float64 = 1.0\n",
    "            descent_direction = H\\grad\n",
    "            guess = Guess_Array[i-1] - descent_direction\n",
    "            while true\n",
    "                if f(Guess_Array[i-1])-f(guess) >= threshold\n",
    "                    break\n",
    "                end\n",
    "                n += 1\n",
    "                guess = Guess_Array[i-1] - beta ^ n .* descent_direction\n",
    "                if n >= 50\n",
    "                    println(\"\"\"Iteration stalled \\n\n",
    "                           Current Derivative Error: $(norm(gradient(guess),2))\\n\n",
    "                           Current Guess: $(guess)\\n\n",
    "                           Newton Steps: $(Newton_Steps)\\n\n",
    "                           Gradient_Steps: $(Gradient_Steps)\n",
    "                        \"\"\")\n",
    "                    return Guess_Array[1:i-1]\n",
    "                end\n",
    "            end\n",
    "        #Gradient Descent Step\n",
    "        else\n",
    "            Gradient_Steps += 1\n",
    "            j::Int64 = 0\n",
    "            val = f(Guess_Array[i-1])\n",
    "            grad = gradient(Guess_Array[i-1])\n",
    "            guess = Guess_Array[i-1] - grad\n",
    "            while true\n",
    "                if f(Guess_Array[i-1])-f(guess) >= threshold\n",
    "                    break\n",
    "                end\n",
    "                j += 1\n",
    "                guess = Guess_Array[i-1] - beta ^ j .* grad\n",
    "                if j >= 50\n",
    "                    println(\"\"\"Iteration stalled \\n\n",
    "                           Current Derivative Error: $(norm(gradient(guess),2))\\n\n",
    "                           Current Guess: $(guess)\\n\n",
    "                           Newton Steps: $(Newton_Steps)\\n\n",
    "                           Gradient_Steps: $(Gradient_Steps)\n",
    "                        \"\"\")\n",
    "                    return Guess_Array[1:i-1]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        Guess_Array[i] = guess\n",
    "        if norm(gradient(guess),2) <= tol\n",
    "            println(\"\"\"Tolerance achieved\\n \n",
    "            Current Derivative Error: $(norm(Guess_Array[i],2))\\n\n",
    "            Current Guess: $(Guess_Array[i])\\n\n",
    "            Newton Steps: $(Newton_Steps)\\n\n",
    "            Gradient_Steps: $(Gradient_Steps)\n",
    "            \"\"\")\n",
    "            return Guess_Array[1:i]\n",
    "        end\n",
    "    end\n",
    "    println(\"\"\"Tolerance not achieved\\n \n",
    "    Current Derivative Error: $(norm(Guess_Array[end],2))\\n\n",
    "    Current Guess: $(Guess_Array[end])\\n\n",
    "    Newton Steps: $(Newton_Steps)\\n\n",
    "    Gradient_Steps: $(Gradient_Steps)\n",
    "    \"\"\")\n",
    "    return Guess_Array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance achieved\n",
      " \n",
      "Current Derivative Error: 0.0002733036915201665\n",
      "\n",
      "Current Guess: [-0.0002269651108198975, -7.739675926472095e-5, -0.00013111631448595867]\n",
      "\n",
      "Newton Steps: 4\n",
      "\n",
      "Gradient_Steps: 0\n",
      "\n",
      "[1.0, 0.6666666666666669, 0.31372549019607865, 0.05159892418258675, 0.0002733036915201665]\n",
      "\n",
      "Without the Newton Steps\n",
      "\n",
      "[1.0, 0.8956125353985525, 0.8240661647794175, 0.7305118872863658, 0.5934410737480493, 0.514008920569273, 0.4620387465762699, 0.37192855223996374, 0.3199834403357785, 0.31808484791659647, 0.31542959614536936, 0.3117406332437549, 0.3066624145006328, 0.29975952700152014, 0.29053582927061883, 0.27848932342513466, 0.26321628218947885, 0.24456232302278036, 0.22278259664766673, 0.19863115887729688, 0.17329325090623535, 0.14814403809296445, 0.12443220108283004, 0.10304068188151136, 0.08441227681811815, 0.06861634097995985, 0.05547595116159741, 0.044688551447049474, 0.0359116444984319, 0.028812733346112503, 0.023093259780664163, 0.0184967818242527, 0.014808818386590521, 0.011852901012644256, 0.009485318432928179, 0.007589790946438665, 0.006072619756649066, 0.00485849890158726, 0.003887005556567324, 0.003109710156518974, 0.002487822254755781, 0.0019902855198902594, 0.0015922426071251022, 0.001273801351790184, 0.0010190448017322461, 0.0008152377461997532, 0.000652191172231202, 0.0005217534371260987, 0.0004174030053642557]\n"
     ]
    }
   ],
   "source": [
    "nums = 2 .* rand(Float64, 3) .- 1\n",
    "nums = nums/norm(nums,2)\n",
    "println([norm(x,2) for x in armijo_alternating(ex,nums; max_iter = 1000)])\n",
    "println(\"\\nWithout the Newton Steps\\n\")\n",
    "println([norm(x,2) for x in armijo(ex,nums; max_iter = 1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Methods\n",
    "Trust region methods overcome the problems that line search methods encounter with non-spd approximate Hessians. In particular, a Newton trust region strategy allows the use of complete Hessian information even in regions where the Hessian has negative curvature. The specific trust region methods we will present effect a smooth transition from the steepest descent direction to the Newton direction in a way that gives the global convergence properties of steepest descent and the fast local convergence of Newtonâ€™s method. How do they work?\n",
    "\n",
    "Suppose our guess is $x_i$. Then, we take a ball around $x_i$ with radius $r$. We then estimate the minimmum of the local quadratic model in the ball, as well as finding the minimum along the boundary of the ball. Then, we update the $x_{i+1}$ and radius $r$. The goal of these methods is to provide a smooth transition from gradient descent to Newtons Method. This gives us good performanc when far away from the region as well as speedy local convergence once the standard assumptions are met.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
