{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton Methods\n",
    "\n",
    "The goal of Quasi-Newton methods is to perform a Newton iteration with some approximation of the Hessian instead of the actual Hessian. Like mentioned in the gradient methods section, it has to do with defining an $H_c$ that works for our newest guess.\n",
    "\n",
    "### BFGS\n",
    "\n",
    "BFGS uses the following scheme to update the hessian estimate at each guess point. After we know our new guess point, we want to solve the following equation for H.\n",
    "\n",
    "$$H(x_{n+1} - x_{n}) = \\nabla f(x_{n+1}) - \\nabla f(x_{n}) \\rightarrow Hs = y$$\n",
    "\n",
    "These equations are refered to as the secant equations as they reduce to the secant method in one dimension. In higher dimensions, the BFGS method uses the following update scheme\n",
    "\n",
    "\n",
    "$$H_{n+1} = H + \\frac{yy^T}{y^Ts} - \\frac{(H_ns)(H_ns)^T}{s^TH_ns}$$\n",
    "\n",
    "#### Global Theory\n",
    "\n",
    "There is not guarantee that this method will converge in the global situtuation. This is escpeciallly true if the Hessian near the minimum differs substantially from the Hessian approximation. Howeever, in practice, this algorithm will frequently show global superlinear convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "name": "julia"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
